{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#https://www.kaggle.com/mlg-ulb/creditcardfraud\n#Credit card fraud\n\n#learned from https://www.kaggle.com/abdelhai/the-power-of-eda-90-accuracy-90-recall\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.cluster import KMeans\nimport sklearn\n%matplotlib inline\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        data=pd.read_csv(os.path.join(dirname, filename))\n        \ndata.head()\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation heapmap \nplt.figure(figsize=(30,20))\ncor=data.corr()\nsns.heatmap(cor,annot=True,cmap=plt.cm.Blues)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nA value of 1 indicating a very strong positive relation between the two variables, \nwhile a value of -1 indicating a very strong negative relation, and a value of 0 indicates no relation\n\nlooking into the relation between the independent variables and the dependent variable Class, \nand see which variables is highly correlated with our target variable the Class. \n\nFrom the visualization, the top 3 independent variables are {V12,V14,V17}.\n"},{"metadata":{},"cell_type":"markdown","source":"**Scaterplot \n**\nPloting each one of the interesting variables against a very boring variable V13 and color each data point with corresponding to it's label (Fraud or Normal)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=data['V13'], y=data['V17'], hue=data['Class'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=data['V13'], y=data['V14'], hue=data['Class'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=data['V13'], y=data['V12'], hue=data['Class'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From all three plots, we can observe a horizontal line that classifies almost all data points into their correct Class value. In other words, if we look at the y-axis that corresponds to one of the interesting variables {V17, V14, V12}, then we can see that most fraudulent data points are located below the value of -5 and normal data points are located above.\n\nIn contrast, if we look at the x-axis where the boring variable is, then both fraudulent and normal data points are almost equally distributed between -4 and 4, which means that we can't draw a vertical line to separate the two groups.\n\nIs that good enough?\n\nNo. Although scatter plots gave us a general idea of what is going on, but they are not precise. So how we can take a better view?"},{"metadata":{},"cell_type":"markdown","source":"**Kernel density estimation (KDE) plots\n**\nTake each one of the interesting variables and approximate the underlying probability density function for each Class value (Frauds Vs. Normal) using kernel density estimation. This should give us a clear idea of how fraudulent and normal datapoints (credit card transactions) are distributed along each variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot KDE for V17 values that belong to Class 0 (Normal)\nsns.kdeplot(data=data[data['Class']==0]['V17'],label=\"Class 0\",shade=True)\n# Plot KDE for V17 values that belong to Class 1 (Fraud)\nsns.kdeplot(data=data[data['Class']==1]['V17'],label=\"Class 1\",shade=True)\nplt.legend() #didn't have legends showing before adding this line\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot KDE for V14 values that belong to Class 0 (Normal)\nsns.kdeplot(data=data[data['Class']==0]['V14'],label=\"Class 0\",shade=True)\n# Plot KDE for V14 values that belong to Class 1 (Fraud)\nsns.kdeplot(data=data[data['Class']==1]['V14'],label=\"Class 1\",shade=True)\nplt.legend() #didn't have legends showing before adding this line\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot KDE for V12 values that belong to Class 0 (Normal)\nsns.kdeplot(data=data[data['Class']==0]['V12'],label=\"Class 0\",shade=True)\n# Plot KDE for V12 values that belong to Class 1 (Fraud)\nsns.kdeplot(data=data[data['Class']==1]['V12'],label=\"Class 1\",shade=True)\nplt.legend() #didn't have legends showing before adding this line\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's summarize our findings in the following points:\n\nIn every interesting variable, the distribution of the normal transaction takes a shape that very close to the standard normal distribution.\nIn every interesting variable, the distribution of the fraudulent transaction takes a shape that very close to a normal distribution with height standard deviation (highly spread).\nIn the boring variable, both fraudulent and normal transactions have the same distribution. close to standard normal distribution.\n\nNow let's display some statistical measurements to support our findings.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Values of the variable V14 that belong to Class 0 (Normal)\ndata[data['Class'] == 0]['V14'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Values of the variable V14 that belong to Class 1 (Fraud)\ndata[data['Class'] == 1]['V14'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The true lables\ny = data.Class\n\n# This is our classifier \nhigh_accuracy_y = [0 if i>-4 and i<4 else 1 for i in data['V14']]\n\n# Calculate accuracy\naccuracy_score(y, high_accuracy_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, from these two statistic findings, by only using the variable V14 and only one line of code, I can achieve an accuracy of 99% predicting whether a credit card transaction is fraudulent or normal.\n\n**However, one thing to consider\n**This data set is highly imbalanced, and the positive class (frauds) account for only 0.172% of all transactions.\n\nWhat does that mean?\n\nWell, the accuracy metric is not a fair measurement of the performance of my classifier, because if I classified all negative (normal) transactions correctly, then I will achieve a very high accuracy since almost all data points belong to this class, and it does not matter how many data points from the other class I classified correctly. That is exactly what I did, I figured out from the KDE that most normal transactions have a V14 value between 4 and -4.\n\nSolution?\n\nWe should consider another metric to measure how many data points from the positive class did we classify correctly, and that metric is called Recall. So let's display the confusion matrix and calculate the Recall."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display confusion matrix for our high accuracy classifier\nsns.set(font_scale=3)\nconfusion_matrix = sklearn.metrics.confusion_matrix(y, high_accuracy_y)\n\nplt.figure(figsize=(16, 14))\nsns.heatmap(confusion_matrix, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\nplt.title(\"Confusion matrix\", fontsize=30)\nplt.ylabel('True label', fontsize=25)\nplt.xlabel('Clustering label', fontsize=25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TN = confusion_matrix[0][0] # True Negative\nFP = confusion_matrix[0][1] # False Positive\nFN = confusion_matrix[1][0] # False Negative\nTP = confusion_matrix[1][1] # True Positive\n# Recall\nTP/(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got a Recall of 76%, which is not that bad, but I claim that we can do better. But how can we classify more of the positive transactions correctly?"},{"metadata":{},"cell_type":"markdown","source":"The idea is to narrow down the interval for the negative transactions. By doing that, we are going to miss classify some of the negative transactions which will decrease the accuracy. But in return, we will increase the number of correctly classified positive transactions which increases the recall. It's a tradeoff! If you look at the KDE of V14 this will make much more sense."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is our high recall classifier \nhigh_recall_y = [0 if i>-1.05 and i<3 else 1 for i in data['V14']]\n\naccuracy_score(y, high_recall_y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display confusion matrix for our high recall classifier\nsns.set(font_scale=3)\nconfusion_matrix = sklearn.metrics.confusion_matrix(y, high_recall_y)\n\nplt.figure(figsize=(16, 14))\nsns.heatmap(confusion_matrix, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\nplt.title(\"Confusion matrix\", fontsize=30)\nplt.ylabel('True label', fontsize=25)\nplt.xlabel('Clustering label', fontsize=25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TN = confusion_matrix[0][0] # True Negative\nFP = confusion_matrix[0][1] # False Positive\nFN = confusion_matrix[1][0] # False Negative\nTP = confusion_matrix[1][1] # True Positive\n# Recall\nTP/(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have a fair trade-off with 90% for both accuracy and recall. I can continue trying to find a better interval"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data[['V17', 'V14', 'V12']]\nkmeans = KMeans(n_clusters=2, max_iter=3000, n_init=20)\n\"\"\"\nn_init. (default=10)\nNumber of time the k-means algorithm will be run with different centroid seeds. \nThe final results will be the best output of n_init consecutive runs in terms of inertia.\n\nmax_iter(default=300)\nMaximum number of iterations of the k-means algorithm for a single run.\n\n\n\"\"\"\n\n# Fit and then store predictions in y_pred_kmeans\ny_pred_kmeans = kmeans.fit_predict(X)\n# Calculate accuracy\naccuracy_score(y, y_pred_kmeans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display confusion matrix for K-means\nsns.set(font_scale=3)\nconfusion_matrix = sklearn.metrics.confusion_matrix(y, y_pred_kmeans)\n\nplt.figure(figsize=(16, 14))\nsns.heatmap(confusion_matrix, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\nplt.title(\"Confusion matrix\", fontsize=30)\nplt.ylabel('True label', fontsize=25)\nplt.xlabel('Clustering label', fontsize=25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TN = confusion_matrix[0][0] # True Negative\nFP = confusion_matrix[0][1] # False Positive\nFN = confusion_matrix[1][0] # False Negative\nTP = confusion_matrix[1][1] # True Positive\n# Recall\nTP/(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6. Conclusion\nBy a careful exploration of the dataset, we successfully identified the most important independent variables. We also understood their relationship with the dependent variable, and that led us to a simple solution for this dataset using only one variable and one line of code. Moreover, we showed that our elegant solution outperformed a KMeans model built using the top three independent variables."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}